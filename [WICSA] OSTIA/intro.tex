%%\begin{itemize}
%%\item I would follow the path of the abstract, we should probably provide some numbers and info on storm
%%\item mind you we should stress on the innovative aspects of the paper and tech. there is nothing strictly related to it
%%\item we should comment on what could be done with OSTIA in combination with Eclipse Based tech.
%%\end{itemize}
%%%Big data architectures have been gaining momentum in the last few years. For example, Twitter uses complex Stream topologies featuring frameworks like Storm to analyse and learn trending topics from billions of tweets per minute. However, verifying the consistency of said topologies often requires de- ployment on multi-node clusters and can be expensive as well as time consuming. As an aid to designers and developers evaluating their Stream topologies at design-time, we developed OSTIA, that is, ?On-the-fly Storm Topology Inference Analysis?. OSTIA allows reverse-engineering of Storm topologies so that designers and developers may: (a) use previously existing model- driven verification&validation techniques on elicited models; (b) visualise and evaluate elicited models against consistency checks that would only be available at deployment and run-time. We illustrate the uses and benefits of OSTIA on three real-life industrial case studies.
%%%%%%
%%%%%% intro needs a bit of refinement with what we say in the title and a few more definitions should be included (e.g., about streaming and what it represents or why topologies ?are? the Big Data architecture)? Perhaps we should also increase the stress and focus on Quality and deployability aspects (i.e., the main topics of next year?s QoSA) in the intro, and how OSTIA aids at improving these aspects

Big data applications process large amounts of data for the purpose of gaining key business intelligence through complex analytics using machine-learning techniques \cite{bdsurvey, ml4bd}. These applications are receiving increased attention in the last years given their ability to yield competitive advantage by direct investigation of user needs and trends hidden in the enormous quantities of data produced daily by the average Internet user. According to Gartner~\cite{gartner} %\footnote{\url{http://www.gartner.com/newsroom/id/2637615}} 
business intelligence and analytics applications will remain a top focus for Chief-Information Officers (CIOs) of most Fortune 500 companies until at least 2017-2018.
However, the cost of ownership of the systems that process big data analytics are high due to high infrastructure costs, steep learning curves for the different frameworks (such as Apache Storm~\cite{storm},
%\footnote{\url{http://storm.apache.org/}}, 
Apache Spark~\cite{spark}
%\footnote{\url{http://spark.apache.org/}} 
or Apache Hadoop~\cite{hadoop}) involved in designing and developing big data applications
%\footnote{\url{https://hadoop.apache.org/}} 
and complexities in large-scale architectures and their governance within networked organizations.

In our experience with designing and developing big data architectures, we observed that a key complexity lies in quickly and continuously evaluating the effectiveness of such architectures. Effectiveness, in big data terms, means that the architecture as well as the architecting processes and tools are able to support design, deployment, operation, refactoring and subsequent (re-)deployment of architectures continuously and consistently with runtime restrictions imposed by big data development frameworks. Storm, for example, requires the processing elements to represent a Directed-Acyclic-Graph (DAG). In toy topologies (comprising few components), such constraints can be effectively checked manually, however, when the number of components in such architectures increases to real-life industrial scale architectures, it is enormously difficult to verify even these ``simple" structural DAG constraints.
%\textbf{TODO: can we add an example of said consistency checks/issues?} \\
We argue that the above notion of architecture and architecting effectiveness can be maintained through continuous architecting of big data applications consistently with a DevOps organisational structure \cite{ossslr,devops}. In the big data domain, continuous architecting means supporting the continuous and incremental improvement of big data architectural designs - e.g., by changing the topological arrangement of architecture elements or any of their properties such as queue lengths - using a constant stream of analyses on running applications as well as platform and infrastructure monitoring. For example, the industrial parter that aided the evaluation of the results in this paper is currently facing the issue of continuously changing and re-arranging their big data stream processing application to several parameters, for example: (a) types of content that need analysis (multimedia images, audios as opposed to news articles and text); (b) types of users that need recommendation (e.g., governments as opposed to single users). Changing and constantly re-arranging an application's architecture requires constant and \emph{continuous architecting} of architecture elements, their interconnection and their visible properties. Also, providing automated support to this continuous architecting exercise, reduces the (re-)design efforts and increases the speed of  big data architectures' (re-)deployability by saving the effort of running trial-and-error experiments on expensive infrastructure.

This paper's contribution in support of said continuous architecting is twofold: (a) we elaborate a series of design anti-patterns and algorithmic manipulation techniques that would help designers identify problems in their designs; (b) we outline OSTIA, that stands for: ``On-the-fly Static Topology Inference Analysis" - OSTIA allows designers and developers to infer the application architecture through on-the-fly reverse-engineering and architecture recovery \cite{archrec}. During this inference step, OSTIA analyses the architecture to verify whether it is consistent with development restrictions and/or deployment constraints of the underlying development frameworks (e.g., DAG constraints). To do so, OSTIA hardcodes intimate knowledge on the streaming development framework (Storm, in our case) and its dependence structure in the form of a meta-model \cite{mda}. This knowledge is necessary to make sure that elicited topologies are correct, so that models may be used in at least five scenarios: (a) realising an exportable visual representation of the developed topologies; (b) verifying structural constraints on topologies that would only become evident during infrastructure setup or runtime operation; (c) verifying the topologies against anti-patterns \cite{patternoriented2000} that may lower performance and limit deployability/executability; (d) manipulate said topologies to elicit non-obvious structural properties such as linearisation or cascading; (e) finally, use topologies for further analysis, e.g., through model verification \cite{icsoft}. In an effort to offer said support in a DevOps fashion, OSTIA was engineered to act as an architecture recovery mechanism that closes the feedback loop between operational data architectures (Ops phase) and their refactoring phase (Dev phase). Currently, OSTIA focuses on Apache Storm, i.e., one of the most famous and established real-time stream processing engines \cite{storm, toshniwal2014storm}. The core element of Storm, is called \emph{topology}, which represents the architecture of the processing components of the application (from now we use topology and architecture interchangeably).

This paper outlines OSTIA, elaborating major usage scenarios, benefits and limitations. Also, we evaluate OSTIA using case-study research to conclude that OSTIA does in fact provide valuable insights for continuous architecting of streaming-based big data architectures.

%The rest of the paper is structured as follows. Section \ref{ra} outlines our research design. Sections \ref{rs}, \ref{sec:anti-pattern} and \ref{algo} describe OSTIA, discussing the (anti-)patterns it supports and its usage scenarios. Section \ref{eval} evaluates OSTIA while Section \ref{disc} discusses results and evaluation outlining OSTIA limitations and threats to validity. Finally, Sections \ref{rw} and \ref{conc} report related work and conclude the paper.
